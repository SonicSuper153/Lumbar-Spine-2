{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "torch version: 2.5.0\n",
      "torchvision version: 0.20.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data'\n",
    "directories = ['processed_lsd_jpgs', 'processed_osf_jpgs', 'processed_spider_jpgs', 'processed_tseg_jpgs']\n",
    "\n",
    "train_dir = os.path.join(dataset_path, 'train')\n",
    "test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "for d in directories:\n",
    "    os.makedirs(os.path.join(train_dir, d), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, d), exist_ok=True)\n",
    "\n",
    "for d in directories:\n",
    "    image_dir = os.path.join(dataset_path, d)\n",
    "    images = os.listdir(image_dir)\n",
    "\n",
    "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "    for img in train_images:\n",
    "        src_path = os.path.join(image_dir, img)\n",
    "        dest_path = os.path.join(train_dir, d, img)  \n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "    for img in test_images:\n",
    "        src_path = os.path.join(image_dir, img)\n",
    "        dest_path = os.path.join(test_dir, d, img) \n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "print(\"Train-test split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the directory '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/train/random': 867\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_directory(directory):\n",
    "    # Filter files by common image extensions\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')\n",
    "    images = [img for img in os.listdir(directory) if img.endswith(image_extensions)]\n",
    "    \n",
    "    return len(images)\n",
    "\n",
    "# Specify the directory you want to check\n",
    "directory = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/train/random'  # Replace with your directory path\n",
    "\n",
    "image_count = count_images_in_directory(directory)\n",
    "print(f\"Number of images in the directory '{directory}': {image_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/train'\n",
    "test_dir = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS):\n",
    "\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  return train_datalaoder, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x31a137750>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x31a0eec10>,\n",
       " ['processed_lsd_jpgs',\n",
       "  'processed_osf_jpgs',\n",
       "  'processed_spider_jpgs',\n",
       "  'processed_tseg_jpgs',\n",
       "  'random'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]) # can also use automatic transforms using weights.transforms()\n",
    "\n",
    "train_dataloaders, test_dataloaders, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "train_dataloaders, test_dataloaders, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_lsd_jpgs',\n",
       " 'processed_osf_jpgs',\n",
       " 'processed_spider_jpgs',\n",
       " 'processed_tseg_jpgs',\n",
       " 'random']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Create an SSL context that doesn't verify certificates\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Make your request with the modified context\n",
    "url = \"https://example.com\"\n",
    "response = urllib.request.urlopen(url, context=ssl_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/9705y5ws5p31tnxd9bmg3ll00000gn/T/ipykernel_19078/1801364724.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pth_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the EfficientNetV2_M model without pre-trained weights\n",
    "model = models.efficientnet_v2_m(weights=None)\n",
    "\n",
    "# Path to your .pth file (replace with actual path)\n",
    "pth_file = 'efficientnet_v2_m-dc08266a.pth'\n",
    "\n",
    "# Load the saved weights from the .pth file\n",
    "checkpoint = torch.load(pth_file)\n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "set_seeds() \n",
    "\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, \n",
    "              out_features=len(class_names),\n",
    "              bias=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/9705y5ws5p31tnxd9bmg3ll00000gn/T/ipykernel_19078/798647838.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('new_trained_model_msa.pth', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.efficientnet as efficientnet\n",
    "\n",
    "model = models.efficientnet_v2_m(weights=None)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = len(class_names)  # Assuming you have defined `class_names`\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    ")\n",
    "\n",
    "# Move the model to the device (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# (Optional) Load weights if you have a trained model already\n",
    "# Load saved weights from the .pth file (if you want to fine-tune or continue training)\n",
    "model.load_state_dict(torch.load('new_trained_model_msa.pth', map_location=device))\n",
    "\n",
    "# Step 4: Save the model after fine-tuning or training\n",
    "\n",
    "print('Model saved successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 5]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 24, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 24, 112, 112]   (648)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 24, 112, 112]   [32, 24, 112, 112]   (48)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 24, 112, 112]   [32, 24, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 24, 112, 112]   [32, 24, 112, 112]   --                   False\n",
       "│    │    └─FusedMBConv (0)                                  [32, 24, 112, 112]   [32, 24, 112, 112]   (5,232)              False\n",
       "│    │    └─FusedMBConv (1)                                  [32, 24, 112, 112]   [32, 24, 112, 112]   (5,232)              False\n",
       "│    │    └─FusedMBConv (2)                                  [32, 24, 112, 112]   [32, 24, 112, 112]   (5,232)              False\n",
       "│    └─Sequential (2)                                        [32, 24, 112, 112]   [32, 48, 56, 56]     --                   False\n",
       "│    │    └─FusedMBConv (0)                                  [32, 24, 112, 112]   [32, 48, 56, 56]     (25,632)             False\n",
       "│    │    └─FusedMBConv (1)                                  [32, 48, 56, 56]     [32, 48, 56, 56]     (92,640)             False\n",
       "│    │    └─FusedMBConv (2)                                  [32, 48, 56, 56]     [32, 48, 56, 56]     (92,640)             False\n",
       "│    │    └─FusedMBConv (3)                                  [32, 48, 56, 56]     [32, 48, 56, 56]     (92,640)             False\n",
       "│    │    └─FusedMBConv (4)                                  [32, 48, 56, 56]     [32, 48, 56, 56]     (92,640)             False\n",
       "│    └─Sequential (3)                                        [32, 48, 56, 56]     [32, 80, 28, 28]     --                   False\n",
       "│    │    └─FusedMBConv (0)                                  [32, 48, 56, 56]     [32, 80, 28, 28]     (98,848)             False\n",
       "│    │    └─FusedMBConv (1)                                  [32, 80, 28, 28]     [32, 80, 28, 28]     (256,800)            False\n",
       "│    │    └─FusedMBConv (2)                                  [32, 80, 28, 28]     [32, 80, 28, 28]     (256,800)            False\n",
       "│    │    └─FusedMBConv (3)                                  [32, 80, 28, 28]     [32, 80, 28, 28]     (256,800)            False\n",
       "│    │    └─FusedMBConv (4)                                  [32, 80, 28, 28]     [32, 80, 28, 28]     (256,800)            False\n",
       "│    └─Sequential (4)                                        [32, 80, 28, 28]     [32, 160, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 28, 28]     [32, 160, 14, 14]    (94,420)             False\n",
       "│    │    └─MBConv (1)                                       [32, 160, 14, 14]    [32, 160, 14, 14]    (265,320)            False\n",
       "│    │    └─MBConv (2)                                       [32, 160, 14, 14]    [32, 160, 14, 14]    (265,320)            False\n",
       "│    │    └─MBConv (3)                                       [32, 160, 14, 14]    [32, 160, 14, 14]    (265,320)            False\n",
       "│    │    └─MBConv (4)                                       [32, 160, 14, 14]    [32, 160, 14, 14]    (265,320)            False\n",
       "│    │    └─MBConv (5)                                       [32, 160, 14, 14]    [32, 160, 14, 14]    (265,320)            False\n",
       "│    │    └─MBConv (6)                                       [32, 160, 14, 14]    [32, 160, 14, 14]    (265,320)            False\n",
       "│    └─Sequential (5)                                        [32, 160, 14, 14]    [32, 176, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 160, 14, 14]    [32, 176, 14, 14]    (413,192)            False\n",
       "│    │    └─MBConv (1)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (2)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (3)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (4)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (5)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (6)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (7)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (8)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (9)                                       [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (10)                                      [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (11)                                      [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (12)                                      [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    │    └─MBConv (13)                                      [32, 176, 14, 14]    [32, 176, 14, 14]    (479,820)            False\n",
       "│    └─Sequential (6)                                        [32, 176, 14, 14]    [32, 304, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 176, 14, 14]    [32, 304, 7, 7]      (615,244)            False\n",
       "│    │    └─MBConv (1)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (2)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (3)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (4)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (5)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (6)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (7)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (8)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (9)                                       [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (10)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (11)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (12)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (13)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (14)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (15)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (16)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    │    └─MBConv (17)                                      [32, 304, 7, 7]      [32, 304, 7, 7]      (1,412,460)          False\n",
       "│    └─Sequential (7)                                        [32, 304, 7, 7]      [32, 512, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 304, 7, 7]      [32, 512, 7, 7]      (1,792,268)          False\n",
       "│    │    └─MBConv (1)                                       [32, 512, 7, 7]      [32, 512, 7, 7]      (3,976,320)          False\n",
       "│    │    └─MBConv (2)                                       [32, 512, 7, 7]      [32, 512, 7, 7]      (3,976,320)          False\n",
       "│    │    └─MBConv (3)                                       [32, 512, 7, 7]      [32, 512, 7, 7]      (3,976,320)          False\n",
       "│    │    └─MBConv (4)                                       [32, 512, 7, 7]      [32, 512, 7, 7]      (3,976,320)          False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 512, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 512, 7, 7]      [32, 1280, 7, 7]     (655,360)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 5]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 5]              6,405                True\n",
       "============================================================================================================================================\n",
       "Total params: 52,864,761\n",
       "Trainable params: 6,405\n",
       "Non-trainable params: 52,858,356\n",
       "Total mult-adds (Units.GIGABYTES): 171.56\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 10026.87\n",
       "Params size (MB): 211.46\n",
       "Estimated Total Size (MB): 10257.60\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), \n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find tensorboard... installing it.\")\n",
    "    !pip install -q tensorboard\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from going_modular.going_modular.engine import train_step, test_step\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    \n",
    "        writer.add_scalars(main_tag=\"Loss\", \n",
    "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                            \"test_loss\": test_loss},\n",
    "                           global_step=epoch)\n",
    "\n",
    "        writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                           tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                            \"test_acc\": test_acc}, \n",
    "                           global_step=epoch)\n",
    "        \n",
    "        writer.add_graph(model=model, \n",
    "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.7439 | train_acc: 0.8351 | test_loss: 0.3806 | test_acc: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [16:04<32:09, 964.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.3187 | train_acc: 0.9294 | test_loss: 0.2768 | test_acc: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [32:59<16:34, 994.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.2555 | train_acc: 0.9353 | test_loss: 0.2317 | test_acc: 0.9286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [51:52<00:00, 1037.44s/it]\n"
     ]
    }
   ],
   "source": [
    "set_seeds()\n",
    "results = train(model=model,\n",
    "                train_dataloader=train_dataloaders,\n",
    "                test_dataloader=test_dataloaders,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=3,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.7438545823097229, 0.31871131011124315, 0.2555431628535534],\n",
       " 'train_acc': [0.8351293103448276, 0.9294181034482759, 0.9353448275862069],\n",
       " 'test_loss': [0.3806075932724135, 0.2767751520233495, 0.23165373051805155],\n",
       " 'test_acc': [0.921875, 0.9241071428571429, 0.9285714285714286]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.models.efficientnet as efficientnet\n",
    "\n",
    "# Step 1: Load the pre-trained EfficientNetV2_M model without weights\n",
    "model = models.efficientnet_v2_m(weights=None)\n",
    "\n",
    "# Step 2: Freeze the feature extraction layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Step 3: Modify the classifier for your specific problem (with custom number of classes)\n",
    "num_classes = len(class_names)  # Assuming you have defined `class_names`\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    ")\n",
    "\n",
    "# Move the model to the device (e.g., GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# (Optional) Load weights if you have a trained model already\n",
    "# Load saved weights from the .pth file (if you want to fine-tune or continue training)\n",
    "# model.load_state_dict(torch.load('model_final_pakka_weights.pth', map_location=device))\n",
    "\n",
    "# Step 4: Save the model after fine-tuning or training\n",
    "torch.save(model.state_dict(), 'new_trained_model_msa.pth')\n",
    "\n",
    "print('Model saved successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "torch version: 2.5.0\n",
      "torchvision version: 0.20.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['processed_lsd_jpgs',\n",
    " 'processed_osf_jpgs',\n",
    " 'processed_spider_jpgs',\n",
    " 'processed_tseg_jpgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_final_pakka_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model_final_pakka_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/9705y5ws5p31tnxd9bmg3ll00000gn/T/ipykernel_20558/4252826138.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, \n",
    "              out_features=4,\n",
    "              bias=True).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),    \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),      \n",
    "    transforms.Normalize(        \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_path = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/processed_lsd_jpgs/0004.jpg'  # Replace with your image path\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_batch.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)  \n",
    "_, predicted_idx = torch.max(output, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: processed_lsd_jpgs\n"
     ]
    }
   ],
   "source": [
    "predicted_class = class_names[predicted_idx.item()]\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['processed_lsd_jpgs',\n",
    " 'processed_osf_jpgs',\n",
    " 'processed_spider_jpgs',\n",
    " 'processed_tseg_jpgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def predict_image_class(image_path, model, class_names, device='cpu'):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "        _, predicted_idx = torch.max(output, 1)\n",
    "    \n",
    "    predicted_class = class_names[predicted_idx.item()]\n",
    "    \n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0326.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0399.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0378.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0372.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0075.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0451.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0444.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0244.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0080.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0325.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0558.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0164.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0332.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0020.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0208.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0184.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0523.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0194.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0021.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 0083.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: ID19.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: ID38.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID39.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID34.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID04.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID22.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID31.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID17.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: ID07.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID16.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID18.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID37.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: ID36.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: ID30.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID25.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID35.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID13.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID02.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: ID26.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: ID14.jpg -> Predicted class: processed_osf_jpgs\n",
      "Image: 1_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 120_t2.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 50_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 129_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 187_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 16_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 12_t2.jpg -> Predicted class: processed_lsd_jpgs\n",
      "Image: 87_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 214_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 63_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 227_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 123_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 75_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 13_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 149_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 21_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 29_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 252_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 69_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: 217_t2.jpg -> Predicted class: processed_spider_jpgs\n",
      "Image: case_0376.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_1060.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0218.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0398.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0692.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0082.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0982.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0151.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0495.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0804.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0522.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0967.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0673.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0932.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0479.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_1221.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_1203.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0497.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_1058.jpg -> Predicted class: processed_tseg_jpgs\n",
      "Image: case_0481.jpg -> Predicted class: processed_tseg_jpgs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),    \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),      \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def predict_random_image_from_directory(model, class_names, directory, device, num_images=1):\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')\n",
    "    images = [img for img in os.listdir(directory) if img.endswith(image_extensions)]\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(f\"No images found in directory: {directory}\")\n",
    "        return\n",
    "    \n",
    "    selected_images = random.sample(images, min(num_images, len(images)))\n",
    "\n",
    "    for img_file in selected_images:\n",
    "        img_path = os.path.join(directory, img_file)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = preprocess(image)\n",
    "        input_batch = input_tensor.unsqueeze(0).to(device)  \n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "            _, predicted_idx = torch.max(output, 1)\n",
    "            predicted_class = class_names[predicted_idx.item()]\n",
    "\n",
    "        print(f\"Image: {img_file} -> Predicted class: {predicted_class}\")\n",
    "\n",
    "directory1 = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/processed_lsd_jpgs'  \n",
    "directory2 = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/processed_osf_jpgs'  \n",
    "directory3 = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/processed_spider_jpgs'  \n",
    "directory4 = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/processed_tseg_jpgs'  \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "predict_random_image_from_directory(model, class_names, directory1, device, num_images=20)  \n",
    "predict_random_image_from_directory(model, class_names, directory2, device, num_images=20)  \n",
    "predict_random_image_from_directory(model, class_names, directory3, device, num_images=20)  \n",
    "predict_random_image_from_directory(model, class_names, directory4, device, num_images=20)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processed_spider_jpgs'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image_class(img_path, model, class_names, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0149.jpg, Class: processed_lsd_jpgs, Confidence: 98.22%\n",
      "Image: 0104.jpg, Class: processed_lsd_jpgs, Confidence: 96.48%\n",
      "Image: 0269.jpg, Class: processed_lsd_jpgs, Confidence: 99.85%\n",
      "Image: 0379.jpg, Class: processed_lsd_jpgs, Confidence: 71.18%\n",
      "Image: 0133.jpg, Class: processed_lsd_jpgs, Confidence: 92.17%\n",
      "Image: 0390.jpg, Class: processed_lsd_jpgs, Confidence: 97.41%\n",
      "Image: 0212.jpg, Class: processed_lsd_jpgs, Confidence: 97.63%\n",
      "Image: 0061.jpg, Class: processed_lsd_jpgs, Confidence: 97.07%\n",
      "Image: 0414.jpg, Class: processed_lsd_jpgs, Confidence: 65.20%\n",
      "Image: 0297.jpg, Class: processed_lsd_jpgs, Confidence: 89.97%\n",
      "Image: 0307.jpg, Class: processed_lsd_jpgs, Confidence: 96.45%\n",
      "Image: 0427.jpg, Class: processed_lsd_jpgs, Confidence: 97.38%\n",
      "Image: 0546.jpg, Class: processed_lsd_jpgs, Confidence: 95.05%\n",
      "Image: 0367.jpg, Class: processed_lsd_jpgs, Confidence: 86.93%\n",
      "Image: 0121.jpg, Class: processed_lsd_jpgs, Confidence: 94.88%\n",
      "Image: 0220.jpg, Class: processed_lsd_jpgs, Confidence: 97.07%\n",
      "Image: 0362.jpg, Class: processed_lsd_jpgs, Confidence: 94.99%\n",
      "Image: 0025.jpg, Class: processed_lsd_jpgs, Confidence: 53.97%\n",
      "Image: 0075.jpg, Class: processed_lsd_jpgs, Confidence: 96.50%\n",
      "Image: 0510.jpg, Class: processed_lsd_jpgs, Confidence: 97.36%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_image_class2(image_path, model, class_names, device='cpu', confidence_threshold=50):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    tensor = preprocess(image)\n",
    "    batch = tensor.unsqueeze(0)\n",
    "    \n",
    "    batch = batch.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(batch)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        \n",
    "        _, predicted_idx = torch.max(output, 1)\n",
    "        confidence = probabilities[0][predicted_idx].item() * 100  \n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "def predict_images_in_directory(directory_path, model, class_names, device='cpu', confidence_threshold=50):\n",
    "    predictions = {}\n",
    "    \n",
    "    image_files = [f for f in os.listdir(directory_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    selected_files = random.sample(image_files, min(20, len(image_files)))\n",
    "    \n",
    "    for filename in selected_files:\n",
    "        image_path = os.path.join(directory_path, filename)\n",
    "        predicted_class, confidence = predict_image_class2(image_path, model, class_names, device, confidence_threshold)\n",
    "        predictions[filename] = {'class': predicted_class, 'confidence': confidence}\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "directory_path = '/Users/ishmeetsinghnagi/CODES/Lumbar Spine/data/processed_lsd_jpgs'\n",
    "predictions = predict_images_in_directory(directory_path, model, class_names, device='cpu')\n",
    "\n",
    "for image_name, prediction in predictions.items():\n",
    "    print(f\"Image: {image_name}, Class: {prediction['class']}, Confidence: {prediction['confidence']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
